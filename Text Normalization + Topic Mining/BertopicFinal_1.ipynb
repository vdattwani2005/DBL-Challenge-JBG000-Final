{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcc927d4-9f87-4b5c-aea7-bb5539d941cc",
   "metadata": {},
   "source": [
    "Check for GPU to be found by torch and cuda drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "653bbb6e-1294-4ff2-9959-5f94605b35bc",
   "metadata": {},
   "source": [
    "import torch\n",
    "#check wether gpu is recognized and show info\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(\"cuDNN Version:\", torch.backends.cudnn.version())\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4eddefb2-9e14-4f98-b285-5b9114e624eb",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79f2b7b0-fea9-4bac-8353-67281df3bbe4",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import euclidean"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cd9d288a-ebd3-4bc6-8e58-515100065a12",
   "metadata": {},
   "source": [
    "Connect to the db to extract the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eece4c4-819b-4a1b-84d9-46a34db3f62a",
   "metadata": {},
   "source": [
    "conn2 = sqlite3.connect(r\"C:\\Users\\20232788\\Desktop\\DBL-1\\tweets.db\")\n",
    "cursor2 = conn2.cursor()\n",
    "cursor2.execute(\"SELECT id_str, norm_tweets FROM emb_tweet_v2\")\n",
    "\n",
    "# Fetch all the results\n",
    "conv_json = cursor2.fetchall()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "647bec8b-f68a-4a64-a232-eff31c5f3950",
   "metadata": {},
   "source": [
    "tweets_df = pd.DataFrame(conv_json, columns=['id_str', 'text'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61c8443f-2dbe-4516-83c0-89a91f94808d",
   "metadata": {},
   "source": [
    "split = tweets_df.shape[0] // 2\n",
    "df_split_final = tweets_df[3500000:]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8f0f4e0a-b0b3-4a1a-b7b1-5b681abfb70c",
   "metadata": {},
   "source": [
    "Sentence Transformer Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed224047-4b8b-4436-916b-80af4ae614fd",
   "metadata": {},
   "source": [
    "#TEST FOR GPU ACTIVATION\n",
    "# Important, you need to shield your code with if __name__. Otherwise, CUDA runs into issues when spawning new processes.\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the model\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\", device='cuda')\n",
    "\n",
    "    \n",
    "    batch_embeddings = []\n",
    "    for text in tqdm(tweets_df['text'][:500], desc=\"Encoding Tweets\"):\n",
    "        embedding = model.encode([text], device='cuda')\n",
    "        batch_embeddings.append(embedding[0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6477c2e9-2b41-4859-b78c-3e2d7a37b3ca",
   "metadata": {},
   "source": [
    "#Embedding process (up to 6h+ depending on the capacity of your computer)\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\", device='cuda')\n",
    "\n",
    "    \n",
    "    save_dir = '/mnt/c/Users/20232788/Desktop/DBL-1/sentence_transformer_embeddings'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty dictionary to store the embeddings\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    \n",
    "    for index, row in tqdm(df_split_final.iterrows(), total=df_split_final.shape[0], desc=\"Encoding Tweets\"):\n",
    "        text = row['text']\n",
    "        tweet_id = row['id_str']  # Assuming 'id' is the column with the tweet IDs\n",
    "\n",
    "        embedding = model.encode([text], device='cuda')\n",
    "\n",
    "        \n",
    "        embeddings_dict[tweet_id] = embedding[0]\n",
    "\n",
    "        \n",
    "        counter += 1\n",
    "\n",
    "        \n",
    "        if counter % 10000 == 0:\n",
    "            np.savez(os.path.join(save_dir, f'embeddings_{counter+3490001}-{counter+3500000}.npz'), **embeddings_dict)\n",
    "\n",
    "           \n",
    "            embeddings_dict.clear()\n",
    "\n",
    "    \n",
    "    if embeddings_dict:\n",
    "        np.savez(os.path.join(save_dir, f'embeddings_{counter-counter%10000+1}-{counter}.npz'), **embeddings_dict)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "138acf50-c692-4f3c-8663-a24d7ec64ca4",
   "metadata": {},
   "source": [
    "len(tweets_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d863824-471c-4198-9a8a-239ba43057b0",
   "metadata": {},
   "source": [
    "len(embeddings_dict)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b14bc24-91ff-46d1-acce-c549418ef2c1",
   "metadata": {},
   "source": [
    "np.savez(os.path.join(save_dir, f'embeddings_441001-4413045.npz'), **embeddings_dict)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9bc33977-57fc-45e4-ada3-6d0067329404",
   "metadata": {},
   "source": [
    "Test load one file and convert it to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d11f51f2-5d91-4c38-9fd7-d5beace1b985",
   "metadata": {},
   "source": [
    "npz_dir = '/mnt/c/Users/20232788/Desktop/DBL-1/sentence_transformer_embeddings'\n",
    "filename = 'embeddings_10001-20000.npz'\n",
    "file_embeddings_dict = np.load(os.path.join(npz_dir, filename), allow_pickle=True)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "464aa9d2-50d8-4026-ad86-a885c611aaae",
   "metadata": {},
   "source": [
    "keys = file_embeddings_dict.files\n",
    "embeddings = [file_embeddings_dict[key] for key in keys]\n",
    "\n",
    "emb_1 = pd.DataFrame({'id_str': keys, 'embedding': embeddings})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6df2a8dd-f9cd-4a53-ba14-19cef300f3ca",
   "metadata": {},
   "source": [
    "emb_1.info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9a5fd38c-967e-47af-9d88-ffd2c36bb126",
   "metadata": {},
   "source": [
    "Function from directory to list of npz formatted dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "350d657e-328e-4c9e-8b75-d2f836883563",
   "metadata": {},
   "source": [
    "npz_dir = r'C:\\Users\\20232788\\Desktop\\DBL-1\\sentence_transformer_embeddings'\n",
    "# Define a function to load the embeddings from a single npz file\n",
    "def load_embeddings(filename):\n",
    "    if filename.endswith('.npz'):\n",
    "        # Load the embeddings from the npz file\n",
    "        file_embeddings_dict = np.load(os.path.join(npz_dir, filename), allow_pickle=True)\n",
    "        return file_embeddings_dict\n",
    "    else:\n",
    "        return {}\n",
    "# Use a ThreadPoolExecutor to parallelize the loading of the embeddings\n",
    "filenames = os.listdir(npz_dir)\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    npz_files = list(tqdm(executor.map(load_embeddings, filenames), total=len(filenames), desc=\"Loading embeddings\"))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "822fb7c3-b7bc-45a1-84c6-1881cf548c38",
   "metadata": {},
   "source": [
    "npz_files"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "38039d2c-486f-4347-b47d-3e2f974ca628",
   "metadata": {},
   "source": [
    "From npz formatted dictionaries to one single dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c504912-fa8e-454c-91c2-d22f40d6e66a",
   "metadata": {},
   "source": [
    "data_dict = {}\n",
    "\n",
    "for npz_file in tqdm(npz_files, desc=\"Processing files\", unit=\"file\"):\n",
    "   \n",
    "    keys = npz_file.files\n",
    "    embeddings = [npz_file[key] for key in keys]\n",
    "    \n",
    "    for key, embedding in zip(keys, embeddings):\n",
    "        data_dict[key] = embedding\n",
    "\n",
    "np.savez(os.path.join(npz_dir, 'combined_embeddings.npz'), **data_dict)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8371ae1e-7e5b-4b67-b4a3-ae5b4f3df4ea",
   "metadata": {},
   "source": [
    "#add the embeddings to the original df and store it locally for easier later usage\n",
    "tweets_df['embedding'] = tweets_df['id_str'].map(data_dict)\n",
    "tweets_df.to_pickle(r'C:\\Users\\20232788\\Desktop\\DBL-1\\complete_sentences_emb\\tweets_df.pkl')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c608dbd6-86b9-477d-b130-a76d6a080c0c",
   "metadata": {},
   "source": [
    "if tweets_df.isnull().values.any():\n",
    "    print(\"DataFrame contains null values.\")\n",
    "else:\n",
    "    print(\"DataFrame does not contain null values.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2691ad9c-18a6-422e-8d51-d46d32f90afa",
   "metadata": {},
   "source": [
    "<span style=\"font-size:2.5em;\">TOPIC MODELLING</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5648aec2-b2d9-45ad-a1f0-5aaf4a7413a5",
   "metadata": {},
   "source": [
    "tweets_df = pd.read_pickle(r'C:\\Users\\20232788\\Desktop\\DBL-1\\complete_sentences_emb\\tweets_df.pkl')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84362b3e-ca7c-4ca7-ac86-edca21bc3b02",
   "metadata": {},
   "source": [
    "tweets_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd207ccb-56c7-4446-9cd0-2e001b70461e",
   "metadata": {},
   "source": [
    "#split df in half for easier computations (my ram cannot handle 4.4M all at the same time :( ) \n",
    "half = len(tweets_df) // 2\n",
    "tweets_df_fh = tweets_df.iloc[:half, :]\n",
    "tweets_df_sh = tweets_df.iloc[half:, :]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29f1f8c5-1054-4824-85f2-e3a6e327a354",
   "metadata": {},
   "source": [
    "emb_array_sh = tweets_df_sh['embedding'].to_numpy()\n",
    "emb_array_fh = tweets_df_fh['embedding'].to_numpy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "110cacd4-2995-410f-86c5-95f2b65c1cb0",
   "metadata": {},
   "source": [
    "emb_array_fh.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd70229d-9c54-4f18-99af-ff757606d89a",
   "metadata": {},
   "source": [
    "#convert flattened embeddings ndarray\n",
    "sub_array_length = 384\n",
    "\n",
    "emb_array_sh = np.array([np.array(sub_array) for sub_array in emb_array_sh])\n",
    "emb_array_fh = np.array([np.array(sub_array) for sub_array in emb_array_fh])\n",
    "\n",
    "print([emb_array_sh.shape, emb_array_fh.shape])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22c36391-b457-46b4-b7a6-6063ac03894a",
   "metadata": {},
   "source": [
    "del tweets_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e166ffa8-3167-4652-a783-9ef33e20be57",
   "metadata": {},
   "source": [
    "Load the models (repeat topic computation 2 times one for each half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "467f9abd-9175-4fc3-ba22-ecdf0e1c9487",
   "metadata": {},
   "source": [
    "umap_model = UMAP(n_neighbors=30, n_components=5, min_dist=0.0, metric='cosine', random_state=42, low_memory=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0930a8cd-5e3c-48f8-89c4-96f78b8213d7",
   "metadata": {},
   "source": [
    "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2a44bbc-ff83-425f-b0d4-a63b4c74d142",
   "metadata": {},
   "source": [
    "ctfidf_model = ClassTfidfTransformer(bm25_weighting=True, reduce_frequent_words=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a50eeaf-ce99-4295-b873-9894202cd51c",
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\", device='cuda')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e3ec6fe-41fd-436a-9438-abcbd72d84af",
   "metadata": {},
   "source": [
    "topic_model_umap = BERTopic(\n",
    "    embedding_model=model,\n",
    "    umap_model=umap_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    min_topic_size=100\n",
    ")\n",
    "\n",
    "# Fit the BERTopic model to the agnostic comments and their embeddings\n",
    "topics_d, probs_d = topic_model_umap.fit_transform(tweets_df_sh['text'], emb_array_sh)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bdd245d-4687-4d5f-b98a-b215783d5b5e",
   "metadata": {},
   "source": [
    "#save the topic model for later use\n",
    "topic_model_umap.save(r\"C:\\Users\\20232788\\Desktop\\DBL-1\\topic_models_bertopic\\topic_model_umap_2.pkl\", serialization=\"pytorch\", save_ctfidf=True, save_embedding_model=model)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ec43971a-9029-4ab5-9aaf-50652c00cea0",
   "metadata": {},
   "source": [
    "load topic models and clean topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f6a6df3-ef28-4ebe-be3a-48b9df9bb2e9",
   "metadata": {},
   "source": [
    "topic_model_umap_fh = BERTopic.load(r\"C:\\Users\\20232788\\Desktop\\DBL-1\\topic_models_bertopic\\topic_model_umap.pkl\", embedding_model=model)\n",
    "topic_model_umap_sh = BERTopic.load(r\"C:\\Users\\20232788\\Desktop\\DBL-1\\topic_models_bertopic\\topic_model_umap_2.pkl\", embedding_model=model)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd4c7cd-f360-46b0-a050-96de160e9d60",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#topic_model_umap_sh.visualize_topics()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44deaf2a-8454-44d9-a3f6-dbd51b3274ef",
   "metadata": {},
   "source": [
    "topic_model_umap_sh.get_topic_info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8a1e3115-726e-4b8a-b4c8-a3ec2d944e96",
   "metadata": {},
   "source": [
    "Merge topics by distance! explanation of the process: \n",
    "get the topics and their word list, store it in a dictionary.\n",
    "The topics also contain the probabilities for each word (not needed atm so erase them), flattend the lists to a single entry. \n",
    "Compute vector embeddings of all of the entries, calculate the distance (euclidean) and compute mean and std. \n",
    "Set threshold variable, create adjacency lists of the similar topics, simpliy the structure of the lists and merge the topics! \n",
    "Repeat the process until the wanted amount of topics is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f0f0da5f-11e7-4a28-b203-6087d936a9b6",
   "metadata": {},
   "source": [
    "num_topics = len(topic_model_umap_fh_cl.topics_)\n",
    "topic_dicts_fh = {}\n",
    "\n",
    "for i in range(num_topics):\n",
    "    topic_dicts_fh[i] = topic_model_umap_fh_cl.get_topic(i)\n",
    "\n",
    "rep_dict_fh = {k: [item[0] for item in v] for k, v in topic_dicts_fh.items() if isinstance(v, list)}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db871f19-dd8b-478e-9126-a6fa118ca39f",
   "metadata": {},
   "source": [
    "num_topics = len(topic_model_umap_sh_cl.topics_)\n",
    "topic_dicts_sh = {}\n",
    "\n",
    "for i in range(num_topics):\n",
    "    topic_dicts_sh[i] = topic_model_umap_sh_cl.get_topic(i)\n",
    "\n",
    "rep_dict_sh = {k: [item[0] for item in v] for k, v in topic_dicts_sh.items() if isinstance(v, list)}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bab5ea57-30ef-4229-8f4b-d5990b498857",
   "metadata": {},
   "source": [
    "key_vectors_fh = {}\n",
    "for k, words in rep_dict_fh.items():\n",
    "    sentence = \" \".join(words)\n",
    "    vector = model.encode(sentence, device='cuda')\n",
    "    key_vectors_fh[k] = vector\n",
    "\n",
    "# Calculate Euclidean distance between sentence embeddings for each pair of keys\n",
    "distances_fh = {}\n",
    "for k1, v1 in key_vectors_fh.items():\n",
    "    for k2, v2 in key_vectors_fh.items():\n",
    "        if k1 != k2:\n",
    "            distance = euclidean(v1, v2)\n",
    "            distances_fh[(k1, k2)] = distance"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "02a8bd69-545b-44a1-989e-1cef8b867563",
   "metadata": {},
   "source": [
    "key_vectors_sh = {}\n",
    "for k, words in rep_dict_sh.items():\n",
    "    sentence = \" \".join(words)\n",
    "    vector = model.encode(sentence, device='cuda')\n",
    "    key_vectors_sh[k] = vector\n",
    "\n",
    "# Calculate Euclidean distance between sentence embeddings for each pair of keys\n",
    "distances_sh = {}\n",
    "for k1, v1 in key_vectors_sh.items():\n",
    "    for k2, v2 in key_vectors_sh.items():\n",
    "        if k1 != k2:\n",
    "            distance = euclidean(v1, v2)\n",
    "            distances_sh[(k1, k2)] = distance"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e6153461-5595-437e-a0df-3ed63c72558d",
   "metadata": {},
   "source": [
    "mean_distance_fh = np.mean(list(distances_fh.values()))\n",
    "std_distance_fh = np.std(list(distances_fh.values()))\n",
    "mean_distance_sh = np.mean(list(distances_sh.values()))\n",
    "std_distance_sh = np.std(list(distances_sh.values()))\n",
    "print(mean_distance_fh, std_distance_fh)\n",
    "print(mean_distance_sh, std_distance_sh)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b02babbe-6c29-4184-8580-e0e0b1fb7641",
   "metadata": {},
   "source": [
    "threshold_fh = mean_distance_fh - 1.5 * std_distance_fh\n",
    "threshold_sh = mean_distance_sh - 1.5 * std_distance_sh"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b1f60948-4aab-40d8-895d-b7b4f4e1783a",
   "metadata": {},
   "source": [
    "groups_fh = []\n",
    "\n",
    "# Iterate over each item in the dictionary\n",
    "for key, value in distances_fh.items():\n",
    "    # If the value is below the threshold\n",
    "    if value < threshold_fh:\n",
    "        # If this is the first key or it doesn't share a number with the last key in the last group\n",
    "        if not groups_fh or not (key[0] == groups_fh[-1][-1][1] or key[1] == groups_fh[-1][-1][0]):\n",
    "            # Start a new group with the key\n",
    "            groups_fh.append([key])\n",
    "        else:\n",
    "            # Otherwise, add the key to the last group\n",
    "            groups_fh[-1].append(key)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bacdb595-2621-4295-8c60-b00d6e2a558d",
   "metadata": {},
   "source": [
    "# Initialize a set to keep track of the second numbers that have been used\n",
    "used_second_numbers = set()\n",
    "\n",
    "# Iterate over the groups\n",
    "for group in groups_fh:\n",
    "    # Iterate over the keys in the group\n",
    "    for i in range(len(group)):\n",
    "        # If the second number of the key is in the used numbers set, remove the key from the group\n",
    "        if group[i][1] in used_second_numbers:\n",
    "            group.pop(i)\n",
    "        else:\n",
    "            # If the second number of the key is not in the used numbers set and it is less than 10, add it to the set\n",
    "            if group[i][1] < 10:\n",
    "                used_second_numbers.add(group[i][1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "873bd02c-2f59-4456-b196-8b8a46b26a62",
   "metadata": {},
   "source": [
    "groups_sh = []\n",
    "\n",
    "# Iterate over each item in the dictionary\n",
    "for key, value in distances_sh.items():\n",
    "    # If the value is below the threshold\n",
    "    if value < threshold_sh:\n",
    "        # If this is the first key or it doesn't share a number with the last key in the last group\n",
    "        if not groups_sh or not (key[0] == groups_sh[-1][-1][1] or key[1] == groups_sh[-1][-1][0]):\n",
    "            # Start a new group with the key\n",
    "            groups_sh.append([key])\n",
    "        else:\n",
    "            # Otherwise, add the key to the last group\n",
    "            groups_sh[-1].append(key)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "06b001e4-9fa6-4147-8f8e-49a56360dfbc",
   "metadata": {},
   "source": [
    "# Initialize a set to keep track of the second numbers that have been used\n",
    "used_second_numbers = set()\n",
    "\n",
    "# Iterate over the groups\n",
    "for group in groups_sh:\n",
    "    # Iterate over the keys in the group\n",
    "    for i in range(len(group)):\n",
    "        # If the second number of the key is in the used numbers set, remove the key from the group\n",
    "        if group[i][1] in used_second_numbers:\n",
    "            group.pop(i)\n",
    "        else:\n",
    "            # If the second number of the key is not in the used numbers set and it is less than 10, add it to the set\n",
    "            if group[i][1] < 10:\n",
    "                used_second_numbers.add(group[i][1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ac752cf0-3e1a-4676-825f-690e5f2ef8b0",
   "metadata": {},
   "source": [
    "def replace_numbers(lst):\n",
    "    replacements = {}\n",
    "    for sublist in lst:\n",
    "        for i in range(len(sublist)):\n",
    "            left, right = sublist[i]\n",
    "            if left in replacements:\n",
    "                sublist[i] = (replacements[left], right)\n",
    "            else:\n",
    "                replacements[right] = left\n",
    "    return lst"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "018d10e3-7d03-4228-89c7-9a4ff4e69170",
   "metadata": {},
   "source": [
    "replace_numbers(groups_fh)\n",
    "groups_fh = [[lst[0]] for lst in groups_fh if lst]\n",
    "groups_fh = [list(tup) for sublist in groups_fh for tup in sublist]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b7c7deb1-904b-4cdf-928f-3ff705c18242",
   "metadata": {},
   "source": [
    "replace_numbers(groups_sh)\n",
    "groups_sh = [[lst[0]] for lst in groups_sh if lst]\n",
    "groups_sh = [list(tup) for sublist in groups_sh for tup in sublist]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1b14442c-e0b5-46a8-84be-5ac5d7f049fe",
   "metadata": {},
   "source": [
    "numbers_fh = [lst[0] for lst in groups_fh]\n",
    "numbers_sh = [lst[0] for lst in groups_sh]\n",
    "\n",
    "un_num_fh = len(set(numbers_fh))\n",
    "un_num_sh = len(set(numbers_sh))\n",
    "\n",
    "print(f'unique num first half {un_num_fh}')\n",
    "print(f'unique num second half {un_num_sh}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ef6aaea1-0b17-4384-bd4c-659ad5115238",
   "metadata": {},
   "source": [
    "topic_model_umap_fh_cl.merge_topics(tweets_df_fh['text'], groups_fh)\n",
    "topic_model_umap_sh_cl.merge_topics(tweets_df_sh['text'], groups_sh)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f032c802-9372-48b3-8ba5-f5a8c8585f6b",
   "metadata": {},
   "source": [
    "topic_model_umap_sh_cl.get_topic_info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d578a35d-948e-412a-b22d-6901ff5101c6",
   "metadata": {},
   "source": [
    "topic_model_umap_fh_cl.get_topic_info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fc684757-d2cd-4c38-8bd5-b6522da860fb",
   "metadata": {},
   "source": [
    "Manually merge the last remaining topics if some are still similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1973050f-d6e1-4889-aad2-201a0019a355",
   "metadata": {},
   "source": [
    "topic_model_umap_fh_cl.merge_topics(tweets_df_fh['text'], [5, 6])\n",
    "topic_model_umap_sh_cl.merge_topics(tweets_df_sh['text'], [2, 6])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ac843e6b-cf40-47aa-85e3-14c6df1b1e86",
   "metadata": {},
   "source": [
    "topic_model_umap_fh_cl.get_topic_info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "047c361a-bf50-4583-9e79-775232b81c78",
   "metadata": {},
   "source": [
    "topic_model_umap_sh_cl.get_topic_info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ba9e3d5e-0cc0-4ff9-ad73-7a8f4578e824",
   "metadata": {},
   "source": [
    "topic_model_umap_fh_cl.set_topic_labels({-1: \"Customer Services\", 0: \"Flight Cancellation/Refounds\", 1:'Inclusivity/Racism', 2:'Destination', 3:'Food', 4:'Online Booking', 5:'Onboard Services'})\n",
    "topic_model_umap_sh_cl.set_topic_labels({-1: \"Customer Services\", 0: \"Flight Cancellation/Refounds\", 1:'Easyjet Franchise', 2:'Food', 3:'Onboard Services', 4:'Oguna/Lufthansa scandal', 5:'Health Issues'})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9acb956e-b686-436f-8590-65c4a2225383",
   "metadata": {},
   "source": [
    "topic_model_umap_fh_cl.visualize_barchart(custom_labels=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "93a65145-08ee-4c57-a6bf-2fbe212d8853",
   "metadata": {},
   "source": [
    "topic_model_umap_sh_cl.visualize_barchart(custom_labels=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c781b89e-b651-4284-8b5d-d72378353574",
   "metadata": {},
   "source": [
    "Save the models and set custom labels in the dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ea7379c5-f91c-47b3-b109-e0d8fef3868a",
   "metadata": {},
   "source": [
    "topic_model_umap_fh_cl.save(r\"C:\\Users\\20232788\\Desktop\\DBL-1\\topic_models_bertopic\\topic_model_umap_fh_cl.pkl\", serialization=\"pytorch\", save_ctfidf=True, save_embedding_model=model)\n",
    "topic_model_umap_sh_cl.save(r\"C:\\Users\\20232788\\Desktop\\DBL-1\\topic_models_bertopic\\topic_model_umap_sh_cl.pkl\", serialization=\"pytorch\", save_ctfidf=True, save_embedding_model=model)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d449b4ec-6783-4aff-beee-bb4969ed921f",
   "metadata": {},
   "source": [
    "tweets_df_fh['topic'] = topic_model_umap_fh_cl.topics_\n",
    "topic_names = {-1: \"Customer Services\", 0: \"Flight Cancellation/Refounds\", 1:'Inclusivity/Racism', 2:'Destination', 3:'Food', 4:'Online Booking', 5:'Onboard Services'}\n",
    "tweets_df_fh['topic'] = tweets_df_fh['topic'].map(topic_names)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e37d76ca-f027-43df-8da6-8f50ad5c2c26",
   "metadata": {},
   "source": [
    "tweets_df_sh['topic'] = topic_model_umap_sh_cl.topics_\n",
    "topic_names = {-1: \"Customer Services\", 0: \"Flight Cancellation/Refounds\", 1:'Easyjet Franchise', 2:'Food', 3:'Onboard Services', 4:'Oguna/Lufthansa scandal', 5:'Health Issues'}\n",
    "tweets_df_sh['topic'] = tweets_df_sh['topic'].map(topic_names)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e93f838-24f0-4057-9050-903bccf087f9",
   "metadata": {},
   "source": [
    "#merge back the two dfs into the original one\n",
    "tweets_df = pd.concat([tweets_df_fh, tweets_df_sh])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d1e98cb5-0bc0-4841-9e93-553de38c9402",
   "metadata": {},
   "source": [
    "tweets_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7d10891e-8c49-4a00-8515-3a2819455c8a",
   "metadata": {},
   "source": [
    "Store both dfs (one with embeddings the other without locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d4cbee-0db0-49fb-b2da-d0659e8d63d3",
   "metadata": {},
   "source": [
    "tweets_df2 = tweets_df[['id_str', 'text', 'topic']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dc8c8a1-9300-42bd-aa99-be96e9c6d2e1",
   "metadata": {},
   "source": [
    "tweets_df.to_pickle(r'C:\\Users\\20232788\\Desktop\\DBL-1\\complete_sentences_emb\\tweets_df.pkl')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a864f31-262a-4c8e-a562-b420bc0d7ce8",
   "metadata": {},
   "source": [
    "tweets_df2.to_pickle(r'C:\\Users\\20232788\\Desktop\\DBL-1\\complete_sentences_emb\\tweets_df2.pkl')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38c861ba-caf8-4252-9e6e-d146c632d035",
   "metadata": {},
   "source": [
    "topic_model_umap_fh_cl = BERTopic.load(r\"C:\\Users\\20232788\\Desktop\\DBL-1\\topic_models_bertopic\\topic_model_umap_fh_cl.pkl\", embedding_model=model)\n",
    "topic_model_umap_sh_cl = BERTopic.load(r\"C:\\Users\\20232788\\Desktop\\DBL-1\\topic_models_bertopic\\topic_model_umap_sh_cl.pkl\", embedding_model=model)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a9fd14a-0f5a-465d-b8af-a9767902a18e",
   "metadata": {},
   "source": [
    "from scipy.special import rel_entr\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\"Calculate the Kullback-Leibler Divergence between two distributions.\n",
    "    \n",
    "    Args:\n",
    "        p (np.array): The first probability distribution (topic distribution).\n",
    "        q (np.array): The second probability distribution (uniform distribution).\n",
    "        \n",
    "    Returns:\n",
    "        float: The Kullback-Leibler Divergence.\n",
    "    \"\"\"\n",
    "    return np.sum(rel_entr(p, q))\n",
    "\n",
    "def get_uniform_distribution(vocab_size):\n",
    "    \"\"\"Get a uniform distribution for a given vocabulary size.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size (int): The size of the vocabulary.\n",
    "        \n",
    "    Returns:\n",
    "        np.array: The uniform distribution.\n",
    "    \"\"\"\n",
    "    return np.full(vocab_size, 1/vocab_size)\n",
    "\n",
    "def get_topic_distribution(topic_model, topic_id):\n",
    "    \"\"\"Get the word distribution for a given topic.\n",
    "    \n",
    "    Args:\n",
    "        topic_model: The topic model object.\n",
    "        topic_id (int): The topic id.\n",
    "        \n",
    "    Returns:\n",
    "        np.array: The word distribution for the topic, or None if the topic doesn't exist.\n",
    "    \"\"\"\n",
    "    topic = topic_model.get_topic(topic_id)\n",
    "    if topic:\n",
    "        return np.array([weight for _, weight in topic])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def calculate_kld_for_all_topics(topic_model):\n",
    "    \"\"\"Calculate KLD scores for all topics in the topic model.\n",
    "    \n",
    "    Args:\n",
    "        topic_model: The topic model object.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with topic ids as keys and KLD scores as values.\n",
    "    \"\"\"\n",
    "    topic_ids = range(len(topic_model.get_topics()))\n",
    "    vocab_size = len(topic_model.get_topic(0))\n",
    "    uniform_distribution = get_uniform_distribution(vocab_size)\n",
    "    \n",
    "    kld_scores = {}\n",
    "    for topic_id in topic_ids:\n",
    "        topic_distribution = get_topic_distribution(topic_model, topic_id)\n",
    "        if topic_distribution is not None:\n",
    "            kld_score = kl_divergence(topic_distribution, uniform_distribution)\n",
    "            kld_scores[topic_id] = kld_score\n",
    "    \n",
    "    return kld_scores"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d7c28bf-57b7-41f0-bceb-ed0cb6285391",
   "metadata": {},
   "source": [
    "def plot_kld_scores(kld_scores):\n",
    "    \"\"\"Plot KLD scores for all topics except the first one.\n",
    "    \n",
    "    Args:\n",
    "        kld_scores (dict): A dictionary with topic ids as keys and KLD scores as values.\n",
    "    \"\"\"\n",
    "    topics = list(kld_scores.keys())\n",
    "    scores = list(kld_scores.values())\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(topics, scores, color=plt.cm.cool(np.linspace(0, 1, len(scores))))\n",
    "    plt.xlabel('Topic ID')\n",
    "    plt.ylabel('KLD Score')\n",
    "    plt.title('KLD Scores for All Topics')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(topics)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ff55799-15d1-43a5-8bd3-e7f5693c107e",
   "metadata": {},
   "source": [
    "kld_scores_topic_model_umap_fh_cl = calculate_kld_for_all_topics(topic_model_umap_fh_cl)\n",
    "plot_kld_scores(kld_scores_topic_model_umap_fh_cl)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b1acb7e-c6d7-419b-96b2-d792ed2b079c",
   "metadata": {},
   "source": [
    "kld_scores_topic_model_umap_sh_cl = calculate_kld_for_all_topics(topic_model_umap_sh_cl)\n",
    "plot_kld_scores(kld_scores_topic_model_umap_sh_cl)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fb2b3c8-ed6b-4c4c-996b-6524fb4cf7dd",
   "metadata": {},
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings_fh = pca.fit_transform(emb_array_fh)\n",
    "reduced_embeddings_sh = pca.fit_transform(emb_array_sh)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afd4bdc3-7469-49d0-861a-bf11f709877d",
   "metadata": {},
   "source": [
    "fig = topic_model_umap_fh_cl.visualize_documents(tweets_df_fh['text'], reduced_embeddings=reduced_embeddings_fh)\n",
    "fig.write_html(r\"C:\\Users\\20232788\\Desktop\\DBL-1\\topics_image_fh.html\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
