{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a4ee09-9bad-44e7-96c4-47f662f66d2e",
   "metadata": {},
   "source": [
    "Text Normalization for the tweets in the db with use of GPU (only Tue laptop)\n",
    "\n",
    "To make use of the Gpu:\n",
    "*** Create anaconda envirorment ***\n",
    "1- install cuda toolkit 12.1\n",
    "2- install Pytorch cuda dependency\n",
    "3- install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80128106-8615-43c0-8d26-86d76747d2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA RTX A1000 Laptop GPU\n",
      "2.3.0+cu121\n",
      "12.1\n",
      "cuDNN Version: 8801\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import sqlite3\n",
    "import torch\n",
    "import cupy as cp\n",
    "import hashlib\n",
    "import spacy\n",
    "import emoji\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(r'C:\\Users\\20232788\\Desktop\\DBL-1\\tweets.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "#check wether gpu is recognized and show info\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(\"cuDNN Version:\", torch.backends.cudnn.version())\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7501710f-963a-4482-81c4-b333152e9cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_trf')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# Download the model within the Python script\n",
    "spacy.cli.download(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bac0b04-31d4-48f5-8294-831e59bb1bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\20232788/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\20232788/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Redownload stopwords to ensure they are in the correct path\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9611118a-bb56-4b28-9a0e-2538d98c19a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query to fetch id and text\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    t.id_str,\n",
    "    CASE\n",
    "        WHEN t.truncated = 0 THEN t.text\n",
    "        ELSE et.full_text\n",
    "    END AS text\n",
    "FROM\n",
    "    tweets t\n",
    "LEFT JOIN\n",
    "    extended_tweets et ON t.id_str = et.id_str\n",
    "WHERE\n",
    "    t.lang = 'en';\n",
    "\"\"\"\n",
    "cursor.execute(query)\n",
    "tweets = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0efe3fa6-a546-43c4-8eb0-a120f000d7bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "table emb_tweet_v2 already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Create new table for normalized text if it doesn't exist\u001b[39;00m\n\u001b[0;32m      2\u001b[0m create_table_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mCREATE TABLE emb_tweet_v2 (\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m    id_str TEXT PRIMARY KEY,\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m    norm_tweets TEXT\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m);\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_table_query\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOperationalError\u001b[0m: table emb_tweet_v2 already exists"
     ]
    }
   ],
   "source": [
    "#Create new table for normalized text if it doesn't exist\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE emb_tweet_v2 (\n",
    "    id_str TEXT PRIMARY KEY,\n",
    "    norm_tweets TEXT\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb72c71-b05f-4cc7-9541-7800bea6690f",
   "metadata": {},
   "source": [
    "Cleaning of non relevant text for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0242533-9435-4ed6-a9ac-a3234379bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lowercase(text):\n",
    "    return text.lower()\n",
    "    \n",
    "def give_emoji_free_text(text):\n",
    "    \"\"\"\n",
    "    Removes emoji's from tweets\n",
    "    Accepts:\n",
    "        Text (tweets)\n",
    "    Returns:\n",
    "        Text (emoji free tweets)\n",
    "    \"\"\"\n",
    "    emoji_list = [c for c in text if c in emoji.EMOJI_DATA]\n",
    "    clean_text = ' '.join([word for word in text.split() if not any(char in emoji_list for char in word)])\n",
    "    return clean_text\n",
    "\n",
    "def url_free_text(text):\n",
    "    '''\n",
    "    Cleans text from urls\n",
    "    '''\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def get_rid_of_mentions(text):\n",
    "    \"\"\"\n",
    "    Removes mentions from tweets except for a specified list\n",
    "    Accepts:\n",
    "        Text (tweets)\n",
    "    Returns:\n",
    "        Text (tweets without unwanted mentions)\n",
    "    \"\"\"\n",
    "    allowed_mentions = {\n",
    "        'klm', 'airfrance', 'british_airways', 'americanair', 'lufthansa',\n",
    "        'airberlin', 'easyjet', 'ryanair', 'singaporeair', 'qantas',\n",
    "        'etihadairways', 'virginatlantic'\n",
    "    }\n",
    "    \n",
    "    def mention_filter(match):\n",
    "        mention = match.group(0)[1:]  # Remove the '@' and convert to lower case\n",
    "        return match.group(0) if mention in allowed_mentions else ''\n",
    "    \n",
    "    return re.sub(r'@\\w+', mention_filter, text)\n",
    "\n",
    "def remove_rt_prefix(text):\n",
    "    \"\"\"\n",
    "    Removes the 'RT' prefix from tweets\n",
    "    Accepts:\n",
    "        Text (tweets)\n",
    "    Returns:\n",
    "        Text (tweets without the 'RT' prefix)\n",
    "    \"\"\"\n",
    "    return text.lstrip('RT ').lstrip('rt ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e366eec-84c3-45bb-b8e0-aa0d3c7eb9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets = [\n",
    "    (tweet_id, make_lowercase(give_emoji_free_text(url_free_text(get_rid_of_mentions(remove_rt_prefix(text))))))\n",
    "    for tweet_id, text in tweets\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fd3a1b6-6954-4e3b-9cf2-b70d027d022c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4486241"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07c7fe04-8986-434f-acde-4da0f2dc14cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4413045,)]\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT count(id_str) FROM tweets\n",
    "\"\"\"\n",
    "cursor.execute(query)\n",
    "emb_count = cursor.fetchall()\n",
    "print(emb_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7fe09d-2214-4d2a-ab9c-3f6ffa55d9d7",
   "metadata": {},
   "source": [
    "Text Normalization (Tokenization, POS tagging, and lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7851178-1858-4b57-8d94-67d40aaaf6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweets: 100%|███████████████████████████████████████████████| 1729287/1729287 [2:58:27<00:00, 161.50tweet/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 10707.4781262 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the SpaCy model with GPU support and minibatch for faster approach\n",
    "spacy.prefer_gpu()\n",
    "spacy.require_gpu()\n",
    "nlp = spacy.load(\"en_core_web_trf\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# Define the beginning and ending boundaries\n",
    "begin = 2756954  # Starting index\n",
    "end = 2800000  # Ending index\n",
    "\n",
    "tweets_to_process = processed_tweets[begin:]\n",
    "\n",
    "num_batches = math.ceil(len(tweets_to_process) / 1024)\n",
    "\n",
    "commit_threshold = 10000  # Commit after every 1000 records\n",
    "records = []\n",
    "\n",
    "# Initialize tqdm progress bar for the batches\n",
    "with tqdm(total=len(tweets_to_process), desc=\"Processing tweets\", unit=\"tweet\") as progress_bar:\n",
    "    # Process the tweets in minibatches\n",
    "    for batch in spacy.util.minibatch(tweets_to_process, size=50):\n",
    "        # Extract tweet IDs and texts\n",
    "        tweet_ids, texts = zip(*batch)\n",
    "        \n",
    "        # Use nlp.pipe to process each batch of texts on the GPU\n",
    "        docs = list(nlp.pipe(texts, batch_size=1024))\n",
    "        \n",
    "        for doc, tweet_id in zip(docs, tweet_ids):\n",
    "            lemmas = [token.lemma_ for token in doc if token.lemma_ not in stop_words and not token.is_punct]\n",
    "            normalized_text = ' '.join(lemmas)\n",
    "            \n",
    "            # Append the tweet ID and normalized text as a tuple\n",
    "            records.append((tweet_id, normalized_text))\n",
    "            \n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        # Check if the threshold is reached\n",
    "        if len(records) >= commit_threshold:\n",
    "            # Perform batch insert\n",
    "            cursor.executemany(\"INSERT OR REPLACE INTO emb_tweet_v2 (id_str, norm_tweets) VALUES (?, ?)\", records)\n",
    "            conn.commit()\n",
    "            records = []  # Reset the records list\n",
    "\n",
    " #Insert any remaining records that didnt meet the threshold\n",
    "if records:\n",
    "    cursor.executemany(\"INSERT OR REPLACE INTO emb_tweet_v2 (id_str, norm_tweets) VALUES (?, ?)\", records)\n",
    "    conn.commit()\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e8dc685-e1b0-4545-a5ee-c72e1f30015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
